# Spark SQL Data Analysis with AWS S3 and Parquet

This GitHub repository contains Python code that demonstrates data analysis using Spark SQL with data stored in an AWS S3 bucket. It covers various SQL queries and operations performed on the dataset, including caching and working with Parquet formatted data. The code also measures the runtime of different operations for performance evaluation.

## Overview

Apache Spark is a powerful data processing framework that can handle large datasets efficiently. This repository showcases how to work with Spark SQL to perform data analysis on a dataset of home sales.

### Features

- Data ingestion from an AWS S3 bucket.
- Creating temporary views of the data.
- Executing SQL queries to answer specific questions about the dataset.
- Caching and uncaching temporary tables to compare performance.
- Working with Parquet formatted data.
- Timing and comparing the runtime of SQL operations.
